{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e248d728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-community in ./.venv/lib/python3.9/site-packages (0.3.27)\n",
      "Requirement already satisfied: sentence-transformers in ./.venv/lib/python3.9/site-packages (5.1.0)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.9/site-packages (2.8.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in ./.venv/lib/python3.9/site-packages (from langchain-community) (0.3.74)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in ./.venv/lib/python3.9/site-packages (from langchain-community) (0.3.27)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.9/site-packages (from langchain-community) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.9/site-packages (from langchain-community) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.9/site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.9/site-packages (from langchain-community) (3.12.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./.venv/lib/python3.9/site-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.venv/lib/python3.9/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in ./.venv/lib/python3.9/site-packages (from langchain-community) (2.10.1)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in ./.venv/lib/python3.9/site-packages (from langchain-community) (0.4.14)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in ./.venv/lib/python3.9/site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in ./.venv/lib/python3.9/site-packages (from langchain-community) (2.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.venv/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in ./.venv/lib/python3.9/site-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.9)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./.venv/lib/python3.9/site-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in ./.venv/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in ./.venv/lib/python3.9/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2->langchain-community) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2->langchain-community) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2->langchain-community) (2025.8.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (4.55.4)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (0.34.4)\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.7.34)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.7)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.9/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.9/site-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in ./.venv/lib/python3.9/site-packages (from langsmith>=0.1.125->langchain-community) (3.11.2)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in ./.venv/lib/python3.9/site-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in ./.venv/lib/python3.9/site-packages (from langsmith>=0.1.125->langchain-community) (0.24.0)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.9/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.9/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain-community sentence-transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92394ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shenhao/Desktop/RAG_system/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shenhao/Desktop/RAG_system/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Google API\n",
    "genai.configure(api_key=os.getenv('GEMINI_API_KEY'))\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7053b7",
   "metadata": {},
   "source": [
    "# Update the RAG System Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ef54d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-experimental in ./.venv/lib/python3.9/site-packages (0.3.4)\n",
      "Requirement already satisfied: langchain-google-genai in ./.venv/lib/python3.9/site-packages (2.0.0)\n",
      "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in ./.venv/lib/python3.9/site-packages (from langchain-experimental) (0.3.27)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.28 in ./.venv/lib/python3.9/site-packages (from langchain-experimental) (0.3.74)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in ./.venv/lib/python3.9/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.27)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.9/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.9/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.9/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.9/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.12.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./.venv/lib/python3.9/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.venv/lib/python3.9/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in ./.venv/lib/python3.9/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.10.1)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in ./.venv/lib/python3.9/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.4.14)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in ./.venv/lib/python3.9/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in ./.venv/lib/python3.9/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.venv/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in ./.venv/lib/python3.9/site-packages (from langchain<1.0.0,>=0.3.26->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.9)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./.venv/lib/python3.9/site-packages (from langchain<1.0.0,>=0.3.26->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.11.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in ./.venv/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in ./.venv/lib/python3.9/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2025.8.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.1.0)\n",
      "Requirement already satisfied: google-generativeai<0.8.0,>=0.7.0 in ./.venv/lib/python3.9/site-packages (from langchain-google-genai) (0.7.2)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.6 in ./.venv/lib/python3.9/site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.6.6)\n",
      "Requirement already satisfied: google-api-core in ./.venv/lib/python3.9/site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.25.1)\n",
      "Requirement already satisfied: google-api-python-client in ./.venv/lib/python3.9/site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.179.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in ./.venv/lib/python3.9/site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.40.3)\n",
      "Requirement already satisfied: protobuf in ./.venv/lib/python3.9/site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.25.8)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.9/site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.67.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in ./.venv/lib/python3.9/site-packages (from google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in ./.venv/lib/python3.9/site-packages (from google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.70.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in ./.venv/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.74.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in ./.venv/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.62.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.venv/lib/python3.9/site-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.venv/lib/python3.9/site-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.venv/lib/python3.9/site-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in ./.venv/lib/python3.9/site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.6.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.9/site-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in ./.venv/lib/python3.9/site-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.11.2)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in ./.venv/lib/python3.9/site-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in ./.venv/lib/python3.9/site-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.24.0)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.9/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.9/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.3.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in ./.venv/lib/python3.9/site-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in ./.venv/lib/python3.9/site-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in ./.venv/lib/python3.9/site-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in ./.venv/lib/python3.9/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (3.2.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain-experimental langchain-google-genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aaa9232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cohere in ./.venv/lib/python3.9/site-packages (5.17.0)\n",
      "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in ./.venv/lib/python3.9/site-packages (from cohere) (1.12.0)\n",
      "Requirement already satisfied: httpx>=0.21.2 in ./.venv/lib/python3.9/site-packages (from cohere) (0.28.1)\n",
      "Requirement already satisfied: httpx-sse==0.4.0 in ./.venv/lib/python3.9/site-packages (from cohere) (0.4.0)\n",
      "Requirement already satisfied: pydantic>=1.9.2 in ./.venv/lib/python3.9/site-packages (from cohere) (2.11.7)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in ./.venv/lib/python3.9/site-packages (from cohere) (2.33.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in ./.venv/lib/python3.9/site-packages (from cohere) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15 in ./.venv/lib/python3.9/site-packages (from cohere) (0.21.4)\n",
      "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in ./.venv/lib/python3.9/site-packages (from cohere) (2.32.4.20250809)\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in ./.venv/lib/python3.9/site-packages (from cohere) (4.14.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->cohere) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->cohere) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->cohere) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->cohere) (2025.8.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./.venv/lib/python3.9/site-packages (from tokenizers<1,>=0.15->cohere) (0.34.4)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (1.1.7)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.9/site-packages (from httpx>=0.21.2->cohere) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.9/site-packages (from httpx>=0.21.2->cohere) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.9/site-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.9/site-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.9/site-packages (from pydantic>=1.9.2->cohere) (0.4.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.9/site-packages (from anyio->httpx>=0.21.2->cohere) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.9/site-packages (from anyio->httpx>=0.21.2->cohere) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1818638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rank-bm25 in ./.venv/lib/python3.9/site-packages (0.2.2)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.9/site-packages (from rank-bm25) (2.0.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28724661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import cohere\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "class improved_RAG:\n",
    "    def __init__(self, embedding_dim: int = 384):\n",
    "        \"\"\"\n",
    "        Initialize the RAG system.\n",
    "        \"\"\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.index = faiss.IndexFlatL2(embedding_dim)\n",
    "        self.embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "        # This will now store a list of dictionaries, not just strings\n",
    "        self.documents: List[Dict] = []\n",
    "        self.cohere_client = cohere.Client(os.getenv('COHERE_API_KEY'))\n",
    "        self.bm25 = None  # Will be created when documents are added\n",
    "        self.tokenized_docs = []  # Store tokenized documents for BM25\n",
    "\n",
    "        \n",
    "        print(f\"RAG system initialized with embedding dimension: {embedding_dim}\")\n",
    "    \n",
    "    def embed_text_batch(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts using batching.\n",
    "        \"\"\"\n",
    "        embeddings = self.embedding_model.embed_documents(texts)\n",
    "        return np.array(embeddings, dtype='float32')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def generate_answer(self, query: str, retrieved_docs: List[Dict]) -> str:\n",
    "        \"\"\"\n",
    "        Generate an answer based on the query and retrieved documents.\n",
    "        \"\"\"\n",
    "        # Combine the content of the retrieved documents into a single context string\n",
    "        context = \"\\\\n\\\\n\".join([f\"Title: {doc['title']}\\\\n{doc['text']}\" for doc, dist in retrieved_docs])\n",
    "\n",
    "        # Create a prompt for the generative model\n",
    "        prompt = f\"\"\"\n",
    "        Context information is provided below.\n",
    "        ---------------------\n",
    "        {context}\n",
    "        ---------------------\n",
    "        Given the context information and not prior knowledge, answer the query.\n",
    "        Query: {query}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "        # Use a generative model to get the final answer\n",
    "        model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b8b24f",
   "metadata": {},
   "source": [
    "# update the add documents first, as it directly affect the retrieve process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74982299",
   "metadata": {},
   "source": [
    "## since previous we treat one row as a chunk, this means even some information in this article is related, the whole article might contain many irrelevant information to the specific query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292c5e69",
   "metadata": {},
   "source": [
    "### so we need a chunk splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "752ae344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "import time\n",
    "def add_documents(self, documents_df: pd.DataFrame, *, chunk_size: int = 1000, chunk_overlap: int = 150) -> None:\n",
    "    \"\"\"Chunk docs using semantic similarity, embed in batch, build both FAISS and BM25 indices.\"\"\"\n",
    "    \n",
    "    # 1. Initialize a local embedding model instead of Google's.\n",
    "    # This runs entirely on your machine.\n",
    "    print(\"Loading local embedding model for semantic chunking...\")\n",
    "    \n",
    "    # 2. Create the semantic chunker using the local model.\n",
    "    splitter = SemanticChunker(\n",
    "        embeddings=self.embedding_model,\n",
    "        breakpoint_threshold_type=\"percentile\",\n",
    "        breakpoint_threshold_amount=95\n",
    "    )\n",
    "\n",
    "    texts_to_embed = []\n",
    "    chunks_meta = []\n",
    "    tokenized_chunks = []  # For BM25\n",
    "\n",
    "    for _id, title, text in documents_df[['_id', 'title', 'text']].itertuples(index=False, name=None):\n",
    "        # Use semantic chunking instead of fixed-size chunking\n",
    "        chunks = splitter.split_text(text)\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Prepare for embedding\n",
    "            full_text = f\"Title: {title}\\n\\n{chunk}\"\n",
    "            texts_to_embed.append(full_text)\n",
    "            \n",
    "            # Store metadata\n",
    "            chunks_meta.append({\n",
    "                'original_doc_id': _id,\n",
    "                'title': title,\n",
    "                'text': chunk\n",
    "            })\n",
    "            \n",
    "            # Tokenize for BM25\n",
    "            tokenized_text = (title + \" \" + chunk).lower().split()\n",
    "            tokenized_chunks.append(tokenized_text)\n",
    "\n",
    "\n",
    "    # Build FAISS index\n",
    "    embeddings = self.embed_text_batch(texts_to_embed)\n",
    "    self.index.add(embeddings)\n",
    "    \n",
    "    # Build BM25 index\n",
    "    self.tokenized_docs.extend(tokenized_chunks)\n",
    "    self.bm25 = BM25Okapi(self.tokenized_docs)\n",
    "    \n",
    "    # Update documents\n",
    "    self.documents.extend(chunks_meta)\n",
    "    \n",
    "    print(f\"Added {len(chunks_meta)} chunks using semantic chunking. Index size: {self.index.ntotal}\")\n",
    "    print(f\"BM25 index built with {len(self.tokenized_docs)} documents\")\n",
    "\n",
    "improved_RAG.add_documents = add_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d78559",
   "metadata": {},
   "source": [
    "# now update the retrieve, this directly affect the context of LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7e6f894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(self, query: str, k: int = 3) -> List[Tuple[Dict, float]]:\n",
    "    \"\"\"\n",
    "    Retrieve documents using hybrid search with RRF and re-ranking.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        k: Number of documents to return after re-ranking\n",
    "        \n",
    "    Returns:\n",
    "        List of tuples (document_dictionary, relevance_score)\n",
    "    \"\"\"\n",
    "    initial_k = 20  # Number of candidates from each method\n",
    "    \n",
    "    # Step 1: FAISS Vector Search\n",
    "    query_embedding = self.embed_text_batch([query]).reshape(1, -1)\n",
    "    distances, indices = self.index.search(query_embedding, initial_k)\n",
    "    \n",
    "    # Store vector search results with their ranks\n",
    "    vector_results = {}\n",
    "    for rank, (idx, distance) in enumerate(zip(indices[0], distances[0])):\n",
    "        if idx < len(self.documents):\n",
    "            vector_results[idx] = rank + 1  # Rank starts from 1\n",
    "    \n",
    "    # Step 2: BM25 Keyword Search\n",
    "    tokenized_query = query.lower().split()\n",
    "    bm25_scores = self.bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # Get top-k indices from BM25\n",
    "    top_bm25_indices = np.argsort(bm25_scores)[::-1][:initial_k]\n",
    "    \n",
    "    # Store BM25 results with their ranks\n",
    "    bm25_results = {}\n",
    "    for rank, idx in enumerate(top_bm25_indices):\n",
    "        if idx < len(self.documents):\n",
    "            bm25_results[idx] = rank + 1  # Rank starts from 1\n",
    "    \n",
    "    # Step 3: Reciprocal Rank Fusion (RRF)\n",
    "    # RRF formula: score = 1 / (k + rank), where k is a constant (typically 60)\n",
    "    rrf_k = 60\n",
    "    rrf_scores = {}\n",
    "    \n",
    "    # Combine all unique document indices\n",
    "    all_indices = set(vector_results.keys()) | set(bm25_results.keys())\n",
    "    \n",
    "    for idx in all_indices:\n",
    "        score = 0\n",
    "        # Add vector search contribution\n",
    "        if idx in vector_results:\n",
    "            score += 1 / (rrf_k + vector_results[idx])\n",
    "        # Add BM25 contribution\n",
    "        if idx in bm25_results:\n",
    "            score += 1 / (rrf_k + bm25_results[idx])\n",
    "        rrf_scores[idx] = score\n",
    "    \n",
    "    # Sort by RRF score and get top candidates\n",
    "    sorted_indices = sorted(rrf_scores.keys(), key=lambda x: rrf_scores[x], reverse=True)\n",
    "    top_fused_indices = sorted_indices[:initial_k]\n",
    "    \n",
    "    # Prepare candidates for re-ranking\n",
    "    candidates = []\n",
    "    for idx in top_fused_indices:\n",
    "        doc = self.documents[idx]\n",
    "        doc_text = f\"Title: {doc['title']}\\n{doc['text']}\"\n",
    "        candidates.append({\n",
    "            'doc': doc,\n",
    "            'text': doc_text,\n",
    "            'rrf_score': rrf_scores[idx]\n",
    "        })\n",
    "    \n",
    "    # Step 4: Re-rank with Cohere\n",
    "    if len(candidates) == 0:\n",
    "        return []\n",
    "    \n",
    "    documents = [c['text'] for c in candidates]\n",
    "    \n",
    "    rerank_results = self.cohere_client.rerank(\n",
    "        query=query,\n",
    "        documents=documents,\n",
    "        top_n=min(k, len(documents)),\n",
    "        model='rerank-english-v3.0'\n",
    "    )\n",
    "    \n",
    "    # Return final results\n",
    "    results = []\n",
    "    for result in rerank_results.results:\n",
    "        idx = result.index\n",
    "        relevance_score = result.relevance_score\n",
    "        results.append((candidates[idx]['doc'], relevance_score))\n",
    "    \n",
    "    return results\n",
    "\n",
    "improved_RAG.retrieve = retrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fb131e",
   "metadata": {},
   "source": [
    "# update the answer function, more robust and stick to the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a269430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(self, query: str, retrieved_docs: List[Dict]) -> str:\n",
    "        \"\"\"\n",
    "        Generate an answer based on the query and retrieved documents.\n",
    "        \"\"\"\n",
    "        # Combine the content of the retrieved documents into a single context string\n",
    "        context = \"\\\\n\\\\n\".join([f\"Title: {doc['title']}\\\\n{doc['text']}\" for doc, dist in retrieved_docs])\n",
    "\n",
    "        # Create a prompt for the generative model\n",
    "        prompt = f\"\"\"\n",
    "        Context information is provided below.\n",
    "        ---------------------\n",
    "        {context}\n",
    "        ---------------------\n",
    "        Given the context information and not prior knowledge, answer the query.\n",
    "        Query: {query}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "        # Use a generative model to get the final answer\n",
    "        model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "\n",
    "improved_RAG.generate_answer = generate_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d45593b",
   "metadata": {},
   "source": [
    "# test the performance now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878cbccd",
   "metadata": {},
   "source": [
    "## prepare the ragas schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73e82b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9n/v_tm1cxd4r7c_3mh9hb1ww340000gn/T/ipykernel_31592/3282936373.py:18: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG system initialized with embedding dimension: 384\n",
      "Loading local embedding model for semantic chunking...\n",
      "Added 7264 chunks using semantic chunking. Index size: 7264\n",
      "BM25 index built with 7264 documents\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rag_system = improved_RAG()\n",
    "\n",
    "# load the corpus from the csv file\n",
    "df = pd.read_csv('./assets/corpus.csv')\n",
    "\n",
    "rag_system.add_documents(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0e00d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared Evaluation DataFrame:\n",
      "  query-id  title                                          text  \\\n",
      "0  PLAIN-3    NaN       Breast Cancer Cells Feed on Cholesterol   \n",
      "1  PLAIN-4    NaN         Using Diet to Treat Asthma and Eczema   \n",
      "2  PLAIN-5    NaN         Treating Asthma With Plants vs. Pills   \n",
      "3  PLAIN-6    NaN    How Fruits and Vegetables Can Treat Asthma   \n",
      "4  PLAIN-7    NaN  How Fruits and Vegetables Can Prevent Asthma   \n",
      "\n",
      "                                ground_truth_doc_ids  \n",
      "0  [MED-2436, MED-2437, MED-2438, MED-2439, MED-2...  \n",
      "1                     [MED-2441, MED-2472, MED-2444]  \n",
      "2  [MED-2445, MED-2458, MED-2448, MED-2450, MED-2...  \n",
      "3  [MED-2456, MED-2459, MED-2458, MED-5072, MED-2...  \n",
      "4  [MED-2461, MED-2464, MED-2468, MED-2469, MED-2...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load your datasets\n",
    "queries_df = pd.read_csv('./assets/queries.csv')\n",
    "ground_truth_df = pd.read_csv('./assets/train.csv')\n",
    "\n",
    "# The queries.csv has the text, but the _id is the query-id\n",
    "# Let's rename the column for clarity\n",
    "queries_df.rename(columns={'_id': 'query-id'}, inplace=True)\n",
    "\n",
    "# Group the ground truth by query-id to get a list of all correct corpus-ids for each query\n",
    "ground_truth_grouped = ground_truth_df.groupby('query-id')['corpus-id'].apply(list).reset_index()\n",
    "ground_truth_grouped.rename(columns={'corpus-id': 'ground_truth_doc_ids'}, inplace=True)\n",
    "\n",
    "# Merge the query texts with the ground truth document IDs\n",
    "eval_df = pd.merge(queries_df, ground_truth_grouped, on='query-id')\n",
    "\n",
    "print(\"Prepared Evaluation DataFrame:\")\n",
    "print(eval_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3ec1fee",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     retrieved_contexts \u001b[38;5;241m=\u001b[39m [doc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m doc, dist \u001b[38;5;129;01min\u001b[39;00m retrieved_docs_with_dist]\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# 3. Use your RAG system to generate an answer\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     generated_answer \u001b[38;5;241m=\u001b[39m \u001b[43mrag_system\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrieved_docs_with_dist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     ragas_data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: query_text,\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontexts\u001b[39m\u001b[38;5;124m\"\u001b[39m: retrieved_contexts,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m     })\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Convert to a Hugging Face Dataset for RAGAS\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 21\u001b[0m, in \u001b[0;36mgenerate_answer\u001b[0;34m(self, query, retrieved_docs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Use a generative model to get the final answer\u001b[39;00m\n\u001b[1;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mGenerativeModel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemini-2.5-flash\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m~/Desktop/RAG_system/.venv/lib/python3.9/site-packages/google/generativeai/generative_models.py:331\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/RAG_system/.venv/lib/python3.9/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:827\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m    826\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 827\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Desktop/RAG_system/.venv/lib/python3.9/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/RAG_system/.venv/lib/python3.9/site-packages/google/api_core/retry/retry_unary.py:294\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    291\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    293\u001b[0m )\n\u001b[0;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/RAG_system/.venv/lib/python3.9/site-packages/google/api_core/retry/retry_unary.py:147\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    149\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m~/Desktop/RAG_system/.venv/lib/python3.9/site-packages/google/api_core/timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         remaining_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout\n\u001b[1;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/RAG_system/.venv/lib/python3.9/site-packages/google/api_core/grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(callable_)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21merror_remapped_callable\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/RAG_system/.venv/lib/python3.9/site-packages/grpc/_channel.py:1175\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1168\u001b[0m     request: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1173\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1174\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m-> 1175\u001b[0m     state, call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/RAG_system/.venv/lib/python3.9/site-packages/grpc/_channel.py:1162\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._blocking\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1145\u001b[0m state\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target)\n\u001b[1;32m   1146\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msegregated_call(\n\u001b[1;32m   1147\u001b[0m     cygrpc\u001b[38;5;241m.\u001b[39mPropagationConstants\u001b[38;5;241m.\u001b[39mGRPC_PROPAGATE_DEFAULTS,\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registered_call_handle,\n\u001b[1;32m   1161\u001b[0m )\n\u001b[0;32m-> 1162\u001b[0m event \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1163\u001b[0m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_deserializer)\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:388\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:97\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:80\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._internal_latent_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:61\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ragas_data = []\n",
    "\n",
    "# Let's process the first 10 queries for this example\n",
    "for index, row in eval_df.head(10).iterrows():\n",
    "    query_text = row['text']\n",
    "\n",
    "    # 1. Use your RAG system to retrieve document chunks\n",
    "    retrieved_docs_with_dist = rag_system.retrieve(query_text, k=3)\n",
    "\n",
    "    # 2. Extract just the text content for the contexts\n",
    "    retrieved_contexts = [doc['text'] for doc, dist in retrieved_docs_with_dist]\n",
    "\n",
    "    # 3. Use your RAG system to generate an answer\n",
    "    generated_answer = rag_system.generate_answer(query_text, retrieved_docs_with_dist)\n",
    "\n",
    "    ragas_data.append({\n",
    "        \"question\": query_text,\n",
    "        \"contexts\": retrieved_contexts,\n",
    "        \"answer\": generated_answer,\n",
    "        # THE FIX: Add the required 'reference' column with a placeholder\n",
    "        \"reference\": \"\"\n",
    "    })\n",
    "\n",
    "# Convert to a Hugging Face Dataset for RAGAS\n",
    "from datasets import Dataset\n",
    "ragas_dataset = Dataset.from_list(ragas_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bf83fc",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b81f2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting manual evaluation...\n",
      "Evaluated: 1/10\n",
      "Evaluated: 2/10\n",
      "Evaluated: 3/10\n",
      "Evaluated: 4/10\n",
      "Evaluated: 5/10\n",
      "Evaluated: 6/10\n",
      "Evaluated: 7/10\n",
      "Evaluated: 8/10\n",
      "Evaluated: 9/10\n",
      "Evaluated: 10/10\n",
      "\n",
      "=== Evaluation Results ===\n",
      "                                            question  faithfulness  \\\n",
      "0         Breast Cancer Cells Feed on Cholesterol...           1.0   \n",
      "1           Using Diet to Treat Asthma and Eczema...           0.8   \n",
      "2           Treating Asthma With Plants vs. Pills...           1.0   \n",
      "3      How Fruits and Vegetables Can Treat Asthma...           1.0   \n",
      "4    How Fruits and Vegetables Can Prevent Asthma...           1.0   \n",
      "5       Our Tax Dollars Subsidize Unhealthy Foods...           0.5   \n",
      "6            Reducing Arsenic in Chicken and Rice...           1.0   \n",
      "7              How Contaminated Are Our Children?...           0.0   \n",
      "8      Blood Type Diet Perceived as \"Crass Fraud\"...           1.0   \n",
      "9  Why Do Heart Doctors Favor Surgery and Drugs O...           0.6   \n",
      "\n",
      "   answer_relevancy  context_precision  average_score  \n",
      "0               1.0                0.9       0.966667  \n",
      "1               1.0                0.8       0.866667  \n",
      "2               1.0                0.8       0.933333  \n",
      "3               1.0                0.8       0.933333  \n",
      "4               1.0                0.9       0.966667  \n",
      "5               0.6                0.7       0.600000  \n",
      "6               1.0                0.8       0.933333  \n",
      "7               0.3                0.3       0.200000  \n",
      "8               0.8                1.0       0.933333  \n",
      "9               0.8                0.2       0.533333  \n",
      "\n",
      "=== Summary Statistics ===\n",
      "       faithfulness  answer_relevancy  context_precision  average_score\n",
      "count      10.00000         10.000000          10.000000      10.000000\n",
      "mean        0.79000          0.850000           0.720000       0.786667\n",
      "std         0.33483          0.236878           0.261619       0.258295\n",
      "min         0.00000          0.300000           0.200000       0.200000\n",
      "25%         0.65000          0.800000           0.725000       0.666667\n",
      "50%         1.00000          1.000000           0.800000       0.933333\n",
      "75%         1.00000          1.000000           0.875000       0.933333\n",
      "max         1.00000          1.000000           1.000000       0.966667\n"
     ]
    }
   ],
   "source": [
    "# Simplified evaluation approach\n",
    "import os\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "from typing import List, Dict\n",
    "\n",
    "# Configure Gemini\n",
    "genai.configure(api_key=os.getenv('GEMINI_API_KEY'))\n",
    "\n",
    "def evaluate_rag_manually(ragas_data: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Manual evaluation of RAG outputs using Gemini directly\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for item in ragas_data:\n",
    "        question = item['question']\n",
    "        contexts = item['contexts']\n",
    "        answer = item['answer']\n",
    "        \n",
    "        # Evaluate Faithfulness\n",
    "        faithfulness_prompt = f\"\"\"\n",
    "        Question: {question}\n",
    "        Context: {' '.join(contexts)}\n",
    "        Answer: {answer}\n",
    "        \n",
    "        Rate how faithful the answer is to the provided context on a scale of 0-1.\n",
    "        Consider if all claims in the answer can be verified from the context.\n",
    "        Return only a number between 0 and 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        faithfulness_score = float(model.generate_content(faithfulness_prompt).text.strip())\n",
    "        \n",
    "        # Evaluate Answer Relevancy\n",
    "        relevancy_prompt = f\"\"\"\n",
    "        Question: {question}\n",
    "        Answer: {answer}\n",
    "        \n",
    "        Rate how relevant the answer is to the question on a scale of 0-1.\n",
    "        Consider if the answer directly addresses what was asked.\n",
    "        Return only a number between 0 and 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        relevancy_score = float(model.generate_content(relevancy_prompt).text.strip())\n",
    "        \n",
    "        # Evaluate Context Precision\n",
    "        precision_prompt = f\"\"\"\n",
    "        Question: {question}\n",
    "        Context: {' '.join(contexts)}\n",
    "        \n",
    "        Rate how precise and relevant the retrieved context is for answering the question on a scale of 0-1.\n",
    "        Return only a number between 0 and 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        precision_score = float(model.generate_content(precision_prompt).text.strip())\n",
    "        \n",
    "        results.append({\n",
    "            'question': question[:50] + '...',  # Truncate for display\n",
    "            'faithfulness': faithfulness_score,\n",
    "            'answer_relevancy': relevancy_score,\n",
    "            'context_precision': precision_score,\n",
    "            'average_score': (faithfulness_score + relevancy_score + precision_score) / 3\n",
    "        })\n",
    "        \n",
    "        print(f\"Evaluated: {len(results)}/{len(ragas_data)}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Use this function with your ragas_data\n",
    "print(\"Starting manual evaluation...\")\n",
    "evaluation_df = evaluate_rag_manually(ragas_data)\n",
    "\n",
    "print(\"\\n=== Evaluation Results ===\")\n",
    "print(evaluation_df)\n",
    "\n",
    "print(\"\\n=== Summary Statistics ===\")\n",
    "print(evaluation_df[['faithfulness', 'answer_relevancy', 'context_precision', 'average_score']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dff614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in ./.venv/lib/python3.9/site-packages (4.5.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in ./.venv/lib/python3.9/site-packages (from optuna) (1.16.4)\n",
      "Requirement already satisfied: colorlog in ./.venv/lib/python3.9/site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.9/site-packages (from optuna) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from optuna) (25.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in ./.venv/lib/python3.9/site-packages (from optuna) (2.0.43)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.9/site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in ./.venv/lib/python3.9/site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in ./.venv/lib/python3.9/site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in ./.venv/lib/python3.9/site-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
      "Requirement already satisfied: tomli in ./.venv/lib/python3.9/site-packages (from alembic>=1.5.0->optuna) (2.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in ./.venv/lib/python3.9/site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4004487a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'improved_RAG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create a modified version of the RAG class that accepts hyperparameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mTunableRAG\u001b[39;00m(\u001b[43mimproved_RAG\u001b[49m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, embedding_dim: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m768\u001b[39m):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(embedding_dim)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'improved_RAG' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a modified version of the RAG class that accepts hyperparameters\n",
    "class TunableRAG(improved_RAG):\n",
    "    def __init__(self, embedding_dim: int = 384):\n",
    "        super().__init__(embedding_dim)\n",
    "        self.initial_k = 20  # Default value\n",
    "        self.final_k = 3     # Default value\n",
    "        self.temperature = 0.5  # Default value\n",
    "        \n",
    "    def add_documents_with_threshold(self, documents_df: pd.DataFrame, breakpoint_threshold_amount: int = 95) -> None:\n",
    "        \"\"\"Add documents with configurable semantic chunking threshold.\"\"\"\n",
    "        # Clear existing documents and indices\n",
    "        self.documents = []\n",
    "        self.tokenized_docs = []\n",
    "        self.index = faiss.IndexFlatL2(self.embedding_dim)\n",
    "        \n",
    "        # Initialize the Google embeddings for semantic chunking\n",
    "        embeddings = GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/text-embedding-004\",\n",
    "            google_api_key=os.getenv('GEMINI_API_KEY')\n",
    "        )\n",
    "        \n",
    "        # Create semantic chunker with specified threshold\n",
    "        splitter = SemanticChunker(\n",
    "            embeddings=embeddings,\n",
    "            breakpoint_threshold_type=\"percentile\",\n",
    "            breakpoint_threshold_amount=breakpoint_threshold_amount,  # Use the parameter\n",
    "            number_of_chunks=None\n",
    "        )\n",
    "        \n",
    "        texts_to_embed = []\n",
    "        chunks_meta = []\n",
    "        tokenized_chunks = []\n",
    "        \n",
    "        for _id, title, text in documents_df[['_id', 'title', 'text']].itertuples(index=False, name=None):\n",
    "            chunks = splitter.split_text(text)\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                full_text = f\"Title: {title}\\n\\n{chunk}\"\n",
    "                texts_to_embed.append(full_text)\n",
    "                \n",
    "                chunks_meta.append({\n",
    "                    'original_doc_id': _id,\n",
    "                    'title': title,\n",
    "                    'text': chunk\n",
    "                })\n",
    "                \n",
    "                tokenized_text = (title + \" \" + chunk).lower().split()\n",
    "                tokenized_chunks.append(tokenized_text)\n",
    "        \n",
    "        # Build FAISS index\n",
    "        embeddings_array = self.embed_text_batch(texts_to_embed)\n",
    "        self.index.add(embeddings_array)\n",
    "        \n",
    "        # Build BM25 index\n",
    "        self.tokenized_docs.extend(tokenized_chunks)\n",
    "        self.bm25 = BM25Okapi(self.tokenized_docs)\n",
    "        \n",
    "        # Update documents\n",
    "        self.documents.extend(chunks_meta)\n",
    "    \n",
    "    def retrieve_with_params(self, query: str, initial_k: int, final_k: int) -> List[Tuple[Dict, float]]:\n",
    "        \"\"\"Retrieve with configurable k parameters.\"\"\"\n",
    "        # Store old values\n",
    "        old_initial_k = self.initial_k\n",
    "        old_final_k = self.final_k\n",
    "        \n",
    "        # Set new values\n",
    "        self.initial_k = initial_k\n",
    "        self.final_k = final_k\n",
    "        \n",
    "        # Call the original retrieve with modified params\n",
    "        # We need to modify the retrieve function to use self.initial_k and self.final_k\n",
    "        \n",
    "        # FAISS Vector Search\n",
    "        query_embedding = self.embed_text_batch([query]).reshape(1, -1)\n",
    "        distances, indices = self.index.search(query_embedding, initial_k)\n",
    "        \n",
    "        vector_results = {}\n",
    "        for rank, (idx, distance) in enumerate(zip(indices[0], distances[0])):\n",
    "            if idx < len(self.documents):\n",
    "                vector_results[idx] = rank + 1\n",
    "        \n",
    "        # BM25 Keyword Search\n",
    "        tokenized_query = query.lower().split()\n",
    "        bm25_scores = self.bm25.get_scores(tokenized_query)\n",
    "        top_bm25_indices = np.argsort(bm25_scores)[::-1][:initial_k]\n",
    "        \n",
    "        bm25_results = {}\n",
    "        for rank, idx in enumerate(top_bm25_indices):\n",
    "            if idx < len(self.documents):\n",
    "                bm25_results[idx] = rank + 1\n",
    "        \n",
    "        # RRF\n",
    "        rrf_k = 60\n",
    "        rrf_scores = {}\n",
    "        all_indices = set(vector_results.keys()) | set(bm25_results.keys())\n",
    "        \n",
    "        for idx in all_indices:\n",
    "            score = 0\n",
    "            if idx in vector_results:\n",
    "                score += 1 / (rrf_k + vector_results[idx])\n",
    "            if idx in bm25_results:\n",
    "                score += 1 / (rrf_k + bm25_results[idx])\n",
    "            rrf_scores[idx] = score\n",
    "        \n",
    "        sorted_indices = sorted(rrf_scores.keys(), key=lambda x: rrf_scores[x], reverse=True)\n",
    "        top_fused_indices = sorted_indices[:initial_k]\n",
    "        \n",
    "        candidates = []\n",
    "        for idx in top_fused_indices:\n",
    "            doc = self.documents[idx]\n",
    "            doc_text = f\"Title: {doc['title']}\\n{doc['text']}\"\n",
    "            candidates.append({\n",
    "                'doc': doc,\n",
    "                'text': doc_text,\n",
    "                'rrf_score': rrf_scores[idx]\n",
    "            })\n",
    "        \n",
    "        if len(candidates) == 0:\n",
    "            return []\n",
    "        \n",
    "        documents = [c['text'] for c in candidates]\n",
    "        \n",
    "        rerank_results = self.cohere_client.rerank(\n",
    "            query=query,\n",
    "            documents=documents,\n",
    "            top_n=min(final_k, len(documents)),\n",
    "            model='rerank-english-v3.0'\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for result in rerank_results.results:\n",
    "            idx = result.index\n",
    "            relevance_score = result.relevance_score\n",
    "            results.append((candidates[idx]['doc'], relevance_score))\n",
    "        \n",
    "        # Restore old values\n",
    "        self.initial_k = old_initial_k\n",
    "        self.final_k = old_final_k\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_answer_with_temp(self, query: str, retrieved_docs: List[Dict], temperature: float) -> str:\n",
    "        \"\"\"Generate answer with configurable temperature.\"\"\"\n",
    "        context = \"\\\\n\\\\n\".join([f\"Title: {doc['title']}\\\\n{doc['text']}\" for doc, dist in retrieved_docs])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Context information is provided below.\n",
    "        ---------------------\n",
    "        {context}\n",
    "        ---------------------\n",
    "        Given the context information and not prior knowledge, answer the query.\n",
    "        Query: {query}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        \n",
    "        # Configure generation with temperature\n",
    "        generation_config = genai.types.GenerationConfig(\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "        response = model.generate_content(prompt, generation_config=generation_config)\n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44607851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna to optimize RAG hyperparameters.\n",
    "    \"\"\"\n",
    "    # Suggest hyperparameters for this trial\n",
    "    breakpoint_threshold = trial.suggest_int(\"breakpoint_threshold\", 90, 99)\n",
    "    initial_k = trial.suggest_categorical(\"initial_k\", [20,25,30,35,40])\n",
    "    final_k = trial.suggest_int(\"final_k\", 3, 10)\n",
    "    temperature = trial.suggest_categorical(\"temperature\", [0.1, 0.5, 0.9])\n",
    "    \n",
    "    print(f\"\\nTrial {trial.number}: Testing params - threshold={breakpoint_threshold}, \"\n",
    "          f\"initial_k={initial_k}, final_k={final_k}, temp={temperature}\")\n",
    "    \n",
    "    # Initialize RAG system with new parameters\n",
    "    rag_tunable = TunableRAG()\n",
    "    \n",
    "    # Add documents with the specified chunking threshold\n",
    "    print(\"  Adding documents with new chunking...\")\n",
    "    rag_tunable.add_documents_with_threshold(df, breakpoint_threshold_amount=breakpoint_threshold)\n",
    "    \n",
    "    # Prepare evaluation data\n",
    "    eval_results = []\n",
    "    \n",
    "    # Use first 10 queries for faster tuning (you can increase this)\n",
    "    eval_subset = eval_df.head(10)\n",
    "    \n",
    "    for index, row in eval_subset.iterrows():\n",
    "        query_text = row['text']\n",
    "        \n",
    "        # Retrieve with specified parameters\n",
    "        retrieved_docs = rag_tunable.retrieve_with_params(query_text, initial_k, final_k)\n",
    "        \n",
    "        # Generate answer with specified temperature\n",
    "        generated_answer = rag_tunable.generate_answer_with_temp(query_text, retrieved_docs, temperature)\n",
    "        \n",
    "        # Extract contexts for evaluation\n",
    "        contexts = [doc['text'] for doc, _ in retrieved_docs]\n",
    "        \n",
    "        eval_results.append({\n",
    "            'question': query_text,\n",
    "            'contexts': contexts,\n",
    "            'answer': generated_answer\n",
    "        })\n",
    "    \n",
    "    # Evaluate using the same manual evaluation function\n",
    "    print(\"  Evaluating performance...\")\n",
    "    model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "    \n",
    "    faithfulness_scores = []\n",
    "    relevancy_scores = []\n",
    "    precision_scores = []\n",
    "    \n",
    "    for item in eval_results:\n",
    "        question = item['question']\n",
    "        contexts = item['contexts']\n",
    "        answer = item['answer']\n",
    "        \n",
    "        # Evaluate Faithfulness\n",
    "        faithfulness_prompt = f\"\"\"\n",
    "        Question: {question}\n",
    "        Context: {' '.join(contexts)}\n",
    "        Answer: {answer}\n",
    "        \n",
    "        Rate how faithful the answer is to the provided context on a scale of 0-1.\n",
    "        Consider if all claims in the answer can be verified from the context.\n",
    "        Return only a number between 0 and 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            faithfulness = float(model.generate_content(faithfulness_prompt).text.strip())\n",
    "        except:\n",
    "            faithfulness = 0.5  # Default if parsing fails\n",
    "            \n",
    "        # Evaluate Answer Relevancy\n",
    "        relevancy_prompt = f\"\"\"\n",
    "        Question: {question}\n",
    "        Answer: {answer}\n",
    "        \n",
    "        Rate how relevant the answer is to the question on a scale of 0-1.\n",
    "        Consider if the answer directly addresses what was asked.\n",
    "        Return only a number between 0 and 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            relevancy = float(model.generate_content(relevancy_prompt).text.strip())\n",
    "        except:\n",
    "            relevancy = 0.5\n",
    "            \n",
    "        # Evaluate Context Precision\n",
    "        precision_prompt = f\"\"\"\n",
    "        Question: {question}\n",
    "        Context: {' '.join(contexts)}\n",
    "        \n",
    "        Rate how precise and relevant the retrieved context is for answering the question on a scale of 0-1.\n",
    "        Return only a number between 0 and 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            precision = float(model.generate_content(precision_prompt).text.strip())\n",
    "        except:\n",
    "            precision = 0.5\n",
    "        \n",
    "        faithfulness_scores.append(faithfulness)\n",
    "        relevancy_scores.append(relevancy)\n",
    "        precision_scores.append(precision)\n",
    "    \n",
    "    # Calculate average scores\n",
    "    avg_faithfulness = np.mean(faithfulness_scores)\n",
    "    avg_relevancy = np.mean(relevancy_scores)\n",
    "    avg_precision = np.mean(precision_scores)\n",
    "    \n",
    "    # Calculate the objective score using the specified formula\n",
    "    score = (0.5 * avg_relevancy) + (0.4 * avg_precision) + (0.1 * avg_faithfulness)\n",
    "    \n",
    "    print(f\"  Scores - Faithfulness: {avg_faithfulness:.3f}, \"\n",
    "          f\"Relevancy: {avg_relevancy:.3f}, Precision: {avg_precision:.3f}\")\n",
    "    print(f\"  Final Score: {score:.3f}\")\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02d0b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-25 22:03:08,813] A new study created in memory with name: rag_hyperparameter_tuning\n",
      "[W 2025-08-25 22:03:08,815] Trial 0 failed with parameters: {'breakpoint_threshold': 97, 'initial_k': 25, 'final_k': 6, 'temperature': 0.1} because of the following error: NameError(\"name 'TunableRAG' is not defined\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shenhao/Desktop/RAG_system/.venv/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/9n/v_tm1cxd4r7c_3mh9hb1ww340000gn/T/ipykernel_2185/4259532904.py\", line 19, in objective\n",
      "    rag_tunable = TunableRAG()\n",
      "NameError: name 'TunableRAG' is not defined\n",
      "[W 2025-08-25 22:03:08,815] Trial 0 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter optimization...\n",
      "============================================================\n",
      "\n",
      "Trial 0: Testing params - threshold=97, initial_k=25, final_k=6, temp=0.1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TunableRAG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting hyperparameter optimization...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Start with 20 trials, increase to 50-100 for production\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimization Complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/RAG_system/.venv/lib/python3.9/site-packages/optuna/study/study.py:490\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    390\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    398\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 490\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/RAG_system/.venv/lib/python3.9/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/RAG_system/.venv/lib/python3.9/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial_id \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/Desktop/RAG_system/.venv/lib/python3.9/site-packages/optuna/study/_optimize.py:258\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    254\u001b[0m     updated_state \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    257\u001b[0m ):\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trial\u001b[38;5;241m.\u001b[39m_trial_id\n",
      "File \u001b[0;32m~/Desktop/RAG_system/.venv/lib/python3.9/site-packages/optuna/study/_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTrial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial\u001b[38;5;241m.\u001b[39mnumber\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Testing params - threshold=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbreakpoint_threshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial_k=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minitial_k\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, final_k=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_k\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, temp=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemperature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Initialize RAG system with new parameters\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m rag_tunable \u001b[38;5;241m=\u001b[39m \u001b[43mTunableRAG\u001b[49m()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Add documents with the specified chunking threshold\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Adding documents with new chunking...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TunableRAG' is not defined"
     ]
    }
   ],
   "source": [
    "# Create and run the Optuna study\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=\"rag_hyperparameter_tuning\"\n",
    ")\n",
    "\n",
    "# Run optimization with fewer trials for testing (increase n_trials for better results)\n",
    "print(\"Starting hyperparameter optimization...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "study.optimize(\n",
    "    objective, \n",
    "    n_trials=50  # Start with 20 trials, increase to 50-100 for production\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Optimization Complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6193b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏆 BEST HYPERPARAMETERS FOUND:\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No trials are completed yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🏆 BEST HYPERPARAMETERS FOUND:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m40\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param, value \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_params\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m📊 Best Score Achieved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/RAG_system/.venv/lib/python3.9/site-packages/optuna/study/study.py:120\u001b[0m, in \u001b[0;36mStudy.best_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbest_params\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return parameters of the best trial in the study.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_trial\u001b[49m\u001b[38;5;241m.\u001b[39mparams\n",
      "File \u001b[0;32m~/Desktop/RAG_system/.venv/lib/python3.9/site-packages/optuna/study/study.py:156\u001b[0m, in \u001b[0;36mStudy.best_trial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbest_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FrozenTrial:\n\u001b[1;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the best trial in the study.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m \n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_best_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/RAG_system/.venv/lib/python3.9/site-packages/optuna/study/study.py:308\u001b[0m, in \u001b[0;36mStudy._get_best_trial\u001b[0;34m(self, deepcopy)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_multi_objective():\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA single best trial cannot be retrieved from a multi-objective study. Consider \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing Study.best_trials to retrieve a list containing the best trials.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m     )\n\u001b[0;32m--> 308\u001b[0m best_trial \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_study_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# If the trial with the best value is infeasible, select the best trial from all feasible\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# trials. Note that the behavior is undefined when constrained optimization without the\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# violation value in the best-valued trial.\u001b[39;00m\n\u001b[1;32m    313\u001b[0m constraints \u001b[38;5;241m=\u001b[39m best_trial\u001b[38;5;241m.\u001b[39msystem_attrs\u001b[38;5;241m.\u001b[39mget(_CONSTRAINTS_KEY)\n",
      "File \u001b[0;32m~/Desktop/RAG_system/.venv/lib/python3.9/site-packages/optuna/storages/_in_memory.py:252\u001b[0m, in \u001b[0;36mInMemoryStorage.get_best_trial\u001b[0;34m(self, study_id)\u001b[0m\n\u001b[1;32m    249\u001b[0m best_trial_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_studies[study_id]\u001b[38;5;241m.\u001b[39mbest_trial_id\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_trial_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo trials are completed yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_studies[study_id]\u001b[38;5;241m.\u001b[39mdirections) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial can be obtained only for single-objective optimization.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    256\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: No trials are completed yet."
     ]
    }
   ],
   "source": [
    "# Display the best parameters found\n",
    "print(\"\\n🏆 BEST HYPERPARAMETERS FOUND:\")\n",
    "print(\"-\" * 40)\n",
    "for param, value in study.best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\n📊 Best Score Achieved: {study.best_value:.4f}\")\n",
    "\n",
    "# Show optimization history\n",
    "print(\"\\n📈 Optimization History (Top 5 Trials):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Get trial dataframe and sort by value\n",
    "trials_df = study.trials_dataframe()\n",
    "top_trials = trials_df.nlargest(5, 'value')[['number', 'value', 'params_breakpoint_threshold', \n",
    "                                               'params_initial_k', 'params_final_k', 'params_temperature']]\n",
    "print(top_trials.to_string())\n",
    "\n",
    "# Create a visualization of the optimization history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot optimization history\n",
    "ax = axes[0, 0]\n",
    "ax.plot(trials_df['number'], trials_df['value'], 'b-', alpha=0.5)\n",
    "ax.scatter(trials_df['number'], trials_df['value'], c='blue', alpha=0.6)\n",
    "ax.set_xlabel('Trial Number')\n",
    "ax.set_ylabel('Objective Score')\n",
    "ax.set_title('Optimization History')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot parameter importance (if enough trials)\n",
    "if len(trials_df) >= 10:\n",
    "    try:\n",
    "        importances = optuna.importance.get_param_importances(study)\n",
    "        ax = axes[0, 1]\n",
    "        params = list(importances.keys())\n",
    "        values = list(importances.values())\n",
    "        ax.barh(params, values)\n",
    "        ax.set_xlabel('Importance')\n",
    "        ax.set_title('Hyperparameter Importance')\n",
    "    except:\n",
    "        axes[0, 1].text(0.5, 0.5, 'Not enough trials for importance analysis', \n",
    "                        ha='center', va='center')\n",
    "\n",
    "# Plot score distribution\n",
    "ax = axes[1, 0]\n",
    "ax.hist(trials_df['value'].dropna(), bins=15, edgecolor='black', alpha=0.7)\n",
    "ax.set_xlabel('Objective Score')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Score Distribution')\n",
    "ax.axvline(study.best_value, color='red', linestyle='--', label=f'Best: {study.best_value:.4f}')\n",
    "ax.legend()\n",
    "\n",
    "# Parameter value counts\n",
    "ax = axes[1, 1]\n",
    "param_counts = {}\n",
    "for param in ['breakpoint_threshold', 'initial_k', 'final_k', 'temperature']:\n",
    "    col = f'params_{param}'\n",
    "    param_counts[param] = trials_df[col].value_counts().to_dict()\n",
    "\n",
    "# Create a summary text\n",
    "summary_text = \"Parameter Value Frequencies:\\n\\n\"\n",
    "for param, counts in param_counts.items():\n",
    "    summary_text += f\"{param}:\\n\"\n",
    "    for value, count in sorted(counts.items()):\n",
    "        summary_text += f\"  {value}: {count} trials\\n\"\n",
    "    summary_text += \"\\n\"\n",
    "\n",
    "ax.text(0.1, 0.5, summary_text, fontsize=10, verticalalignment='center', fontfamily='monospace')\n",
    "ax.set_title('Parameter Usage Summary')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfb5cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf31416b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
