{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92394ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shenhao/Desktop/RAG_system/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shenhao/Desktop/RAG_system/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Google API\n",
    "genai.configure(api_key=os.getenv('GEMINI_API_KEY'))\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7053b7",
   "metadata": {},
   "source": [
    "# Update the RAG System Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28724661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "\n",
    "class improved_RAG:\n",
    "    def __init__(self, embedding_dim: int = 768):\n",
    "        \"\"\"\n",
    "        Initialize the RAG system.\n",
    "        \"\"\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.index = faiss.IndexFlatL2(embedding_dim)\n",
    "        \n",
    "        # This will now store a list of dictionaries, not just strings\n",
    "        self.documents: List[Dict] = []\n",
    "        \n",
    "        print(f\"RAG system initialized with embedding dimension: {embedding_dim}\")\n",
    "    \n",
    "    def embed_text_batch(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts using batching.\n",
    "        \"\"\"\n",
    "        result = genai.embed_content(\n",
    "            model=\"models/text-embedding-004\",\n",
    "            content=texts,\n",
    "            task_type=\"retrieval_document\"\n",
    "        )\n",
    "        return np.array(result['embedding'], dtype='float32')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def generate_answer(self, query: str, retrieved_docs: List[Dict]) -> str:\n",
    "        \"\"\"\n",
    "        Generate an answer based on the query and retrieved documents.\n",
    "        \"\"\"\n",
    "        # Combine the content of the retrieved documents into a single context string\n",
    "        context = \"\\\\n\\\\n\".join([f\"Title: {doc['title']}\\\\n{doc['text']}\" for doc, dist in retrieved_docs])\n",
    "\n",
    "        # Create a prompt for the generative model\n",
    "        prompt = f\"\"\"\n",
    "        Context information is provided below.\n",
    "        ---------------------\n",
    "        {context}\n",
    "        ---------------------\n",
    "        Given the context information and not prior knowledge, answer the query.\n",
    "        Query: {query}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "        # Use a generative model to get the final answer\n",
    "        model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b8b24f",
   "metadata": {},
   "source": [
    "# update the add documents first, as it directly affect the retrieve process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752ae344",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_documents(self, documents_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Add documents from a DataFrame to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            documents_df: A pandas DataFrame with '_id', 'title', and 'text' columns.\n",
    "        \"\"\"\n",
    "        print(f\"Adding {len(documents_df)} documents to the index...\")\n",
    "\n",
    "        # 1. Combine title and text for better embeddings\n",
    "        texts_to_embed = (documents_df['title'] + \" \" + documents_df['text']).tolist()\n",
    "        \n",
    "        # 2. Generate embeddings in a single batch call\n",
    "        embeddings_array = self.embed_text_batch(texts_to_embed)\n",
    "        \n",
    "        # 3. Add embeddings to FAISS index\n",
    "        self.index.add(embeddings_array)\n",
    "        \n",
    "        # 4. Store the original data (as dictionaries)\n",
    "        self.documents.extend(documents_df.to_dict('records'))\n",
    "        \n",
    "        print(f\"Total documents in index: {self.index.ntotal}\")\n",
    "\n",
    "\n",
    "improved_RAG.add_documents = add_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d78559",
   "metadata": {},
   "source": [
    "# now update the retrieve, this directly affect the context of LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e6f894",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve(self, query: str, k: int = 3) -> List[Tuple[Dict, float]]:\n",
    "        \"\"\"\n",
    "        Retrieve the most relevant documents for a query.\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            k: Number of documents to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            List of tuples (document_dictionary, distance)\n",
    "        \"\"\"\n",
    "        # Embed the query (using a single-item list for consistency)\n",
    "        query_embedding = self.embed_text_batch([query]).reshape(1, -1)\n",
    "        \n",
    "        # Search in FAISS index\n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "        \n",
    "        # Prepare results\n",
    "        results = []\n",
    "        for idx, distance in zip(indices[0], distances[0]):\n",
    "            if idx < len(self.documents):\n",
    "                # Retrieve the full dictionary using the index\n",
    "                results.append((self.documents[idx], float(distance)))\n",
    "        \n",
    "        return results\n",
    "\n",
    "improved_RAG.retrieve = retrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fb131e",
   "metadata": {},
   "source": [
    "# update the answer function, more robust and stick to the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a269430",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_answer(self, query: str, retrieved_docs: List[Dict]) -> str:\n",
    "        \"\"\"\n",
    "        Generate an answer based on the query and retrieved documents.\n",
    "        \"\"\"\n",
    "        # Combine the content of the retrieved documents into a single context string\n",
    "        context = \"\\\\n\\\\n\".join([f\"Title: {doc['title']}\\\\n{doc['text']}\" for doc, dist in retrieved_docs])\n",
    "\n",
    "        # Create a prompt for the generative model\n",
    "        prompt = f\"\"\"\n",
    "        Context information is provided below.\n",
    "        ---------------------\n",
    "        {context}\n",
    "        ---------------------\n",
    "        Given the context information and not prior knowledge, answer the query.\n",
    "        Query: {query}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "        # Use a generative model to get the final answer\n",
    "        model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "\n",
    "improved_RAG.generate_answer = generate_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d45593b",
   "metadata": {},
   "source": [
    "# test the performance now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878cbccd",
   "metadata": {},
   "source": [
    "## prepare the ragas schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e82b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rag_system = improved_RAG()\n",
    "\n",
    "# load the corpus from the csv file\n",
    "df = pd.read_csv('./assets/corpus.csv')\n",
    "\n",
    "rag_system.add_documents(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e00d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load your datasets\n",
    "queries_df = pd.read_csv('./assets/queries.csv')\n",
    "ground_truth_df = pd.read_csv('./assets/train.csv')\n",
    "\n",
    "# The queries.csv has the text, but the _id is the query-id\n",
    "# Let's rename the column for clarity\n",
    "queries_df.rename(columns={'_id': 'query-id'}, inplace=True)\n",
    "\n",
    "# Group the ground truth by query-id to get a list of all correct corpus-ids for each query\n",
    "ground_truth_grouped = ground_truth_df.groupby('query-id')['corpus-id'].apply(list).reset_index()\n",
    "ground_truth_grouped.rename(columns={'corpus-id': 'ground_truth_doc_ids'}, inplace=True)\n",
    "\n",
    "# Merge the query texts with the ground truth document IDs\n",
    "eval_df = pd.merge(queries_df, ground_truth_grouped, on='query-id')\n",
    "\n",
    "print(\"Prepared Evaluation DataFrame:\")\n",
    "print(eval_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ec1fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_data = []\n",
    "\n",
    "# Let's process the first 10 queries for this example\n",
    "for index, row in eval_df.head(10).iterrows():\n",
    "    query_text = row['text']\n",
    "\n",
    "    # 1. Use your RAG system to retrieve document chunks\n",
    "    retrieved_docs_with_dist = rag_system.retrieve(query_text, k=3)\n",
    "\n",
    "    # 2. Extract just the text content for the contexts\n",
    "    retrieved_contexts = [doc['text'] for doc, dist in retrieved_docs_with_dist]\n",
    "\n",
    "    # 3. Use your RAG system to generate an answer\n",
    "    generated_answer = rag_system.generate_answer(query_text, retrieved_docs_with_dist)\n",
    "\n",
    "    ragas_data.append({\n",
    "        \"question\": query_text,\n",
    "        \"contexts\": retrieved_contexts,\n",
    "        \"answer\": generated_answer,\n",
    "        # THE FIX: Add the required 'reference' column with a placeholder\n",
    "        \"reference\": \"\"\n",
    "    })\n",
    "\n",
    "# Convert to a Hugging Face Dataset for RAGAS\n",
    "from datasets import Dataset\n",
    "ragas_dataset = Dataset.from_list(ragas_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bf83fc",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b81f2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified evaluation approach\n",
    "import os\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "from typing import List, Dict\n",
    "\n",
    "# Configure Gemini\n",
    "genai.configure(api_key=os.getenv('GEMINI_API_KEY'))\n",
    "\n",
    "def evaluate_rag_manually(ragas_data: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Manual evaluation of RAG outputs using Gemini directly\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for item in ragas_data:\n",
    "        question = item['question']\n",
    "        contexts = item['contexts']\n",
    "        answer = item['answer']\n",
    "        \n",
    "        # Evaluate Faithfulness\n",
    "        faithfulness_prompt = f\"\"\"\n",
    "        Question: {question}\n",
    "        Context: {' '.join(contexts)}\n",
    "        Answer: {answer}\n",
    "        \n",
    "        Rate how faithful the answer is to the provided context on a scale of 0-1.\n",
    "        Consider if all claims in the answer can be verified from the context.\n",
    "        Return only a number between 0 and 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        faithfulness_score = float(model.generate_content(faithfulness_prompt).text.strip())\n",
    "        \n",
    "        # Evaluate Answer Relevancy\n",
    "        relevancy_prompt = f\"\"\"\n",
    "        Question: {question}\n",
    "        Answer: {answer}\n",
    "        \n",
    "        Rate how relevant the answer is to the question on a scale of 0-1.\n",
    "        Consider if the answer directly addresses what was asked.\n",
    "        Return only a number between 0 and 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        relevancy_score = float(model.generate_content(relevancy_prompt).text.strip())\n",
    "        \n",
    "        # Evaluate Context Precision\n",
    "        precision_prompt = f\"\"\"\n",
    "        Question: {question}\n",
    "        Context: {' '.join(contexts)}\n",
    "        \n",
    "        Rate how precise and relevant the retrieved context is for answering the question on a scale of 0-1.\n",
    "        Return only a number between 0 and 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        precision_score = float(model.generate_content(precision_prompt).text.strip())\n",
    "        \n",
    "        results.append({\n",
    "            'question': question[:50] + '...',  # Truncate for display\n",
    "            'faithfulness': faithfulness_score,\n",
    "            'answer_relevancy': relevancy_score,\n",
    "            'context_precision': precision_score,\n",
    "            'average_score': (faithfulness_score + relevancy_score + precision_score) / 3\n",
    "        })\n",
    "        \n",
    "        print(f\"Evaluated: {len(results)}/{len(ragas_data)}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Use this function with your ragas_data\n",
    "print(\"Starting manual evaluation...\")\n",
    "evaluation_df = evaluate_rag_manually(ragas_data)\n",
    "\n",
    "print(\"\\n=== Evaluation Results ===\")\n",
    "print(evaluation_df)\n",
    "\n",
    "print(\"\\n=== Summary Statistics ===\")\n",
    "print(evaluation_df[['faithfulness', 'answer_relevancy', 'context_precision', 'average_score']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb2de27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aae48ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dff614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4004487a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
