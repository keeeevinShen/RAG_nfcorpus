<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RAG Performance Analysis - Technical Details</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 2rem 0;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 2rem;
        }

        .main-content {
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(10px);
            border-radius: 20px;
            padding: 3rem;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
            border: 1px solid rgba(255, 255, 255, 0.2);
        }

        .header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 2px solid #e1e8ed;
        }

        .header h1 {
            font-size: 2.5rem;
            font-weight: 700;
            color: #2c3e50;
            margin-bottom: 0.5rem;
        }

        .header p {
            font-size: 1.1rem;
            color: #7f8c8d;
            font-weight: 300;
        }

        .section {
            margin-bottom: 3rem;
        }

        .section h2 {
            font-size: 1.8rem;
            color: #2c3e50;
            margin-bottom: 1.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .section h3 {
            font-size: 1.4rem;
            color: #34495e;
            margin-bottom: 1rem;
            margin-top: 2rem;
        }

        .section h4 {
            font-size: 1.2rem;
            color: #2980b9;
            margin-bottom: 0.8rem;
            margin-top: 1.5rem;
            font-weight: 600;
        }

        .performance-table {
            width: 100%;
            border-collapse: collapse;
            margin: 2rem 0;
            background: white;
            border-radius: 12px;
            overflow: hidden;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
        }

        .performance-table th {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            padding: 1rem;
            text-align: left;
            font-weight: 600;
            font-size: 0.95rem;
        }

        .performance-table td {
            padding: 1rem;
            border-bottom: 1px solid #e1e8ed;
            transition: background-color 0.3s ease;
        }

        .performance-table tr:hover td {
            background-color: #f8f9fa;
        }

        .performance-table tr:last-child td {
            border-bottom: none;
        }

        .metric-name {
            font-weight: 600;
            color: #2c3e50;
        }

        .score {
            font-weight: 700;
            font-size: 1.1rem;
        }

        .score.excellent {
            color: #27ae60;
        }

        .score.good {
            color: #f39c12;
        }

        .score.average {
            color: #e74c3c;
        }

        .summary-note {
            background: linear-gradient(135deg, #74b9ff, #0984e3);
            color: white;
            padding: 1.5rem;
            border-radius: 12px;
            margin: 1.5rem 0;
            font-style: italic;
        }

        .problem-analysis {
            background: #fff5f5;
            border-left: 4px solid #e53e3e;
            padding: 1.5rem;
            border-radius: 8px;
            margin: 1.5rem 0;
        }

        .problem-analysis h4 {
            color: #e53e3e;
            margin-top: 0;
        }

        .improvement-area {
            background: #f0fff4;
            border-left: 4px solid #48bb78;
            padding: 1.5rem;
            border-radius: 8px;
            margin: 1.5rem 0;
        }

        .improvement-area h4 {
            color: #48bb78;
            margin-top: 0;
        }

        .insights-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2rem;
            margin: 2rem 0;
        }

        .insight-card {
            background: white;
            padding: 2rem;
            border-radius: 12px;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
            border: 1px solid #e1e8ed;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        .insight-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 8px 25px rgba(0, 0, 0, 0.15);
        }

        .insight-card h4 {
            margin-top: 0;
        }

        .highlight {
            background: linear-gradient(120deg, #a8edea 0%, #fed6e3 100%);
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
            font-weight: 600;
        }

        .emoji {
            font-size: 1.2rem;
            margin-right: 0.5rem;
        }

        @media (max-width: 768px) {
            .main-content {
                padding: 2rem 1.5rem;
            }
            
            .header h1 {
                font-size: 2rem;
            }
            
            .performance-table {
                font-size: 0.9rem;
            }
            
            .performance-table th,
            .performance-table td {
                padding: 0.8rem;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="main-content">
            <div class="header">
                <h1>RAG System Performance Analysis</h1>
                <p>Technical evaluation and improvement insights for NFcorpus dataset</p>
                
                <div class="summary-note" style="margin-top: 2rem; text-align: left; background: linear-gradient(135deg, #e3f2fd, #bbdefb); color: #0d47a1; border: 2px solid #2196f3;">
                    <h3 style="margin-top: 0; color: #0d47a1;">üìä Objective Score Definition</h3>
                    <p style="color: #0d47a1;">Throughout this analysis, we use a weighted composite score to evaluate RAG performance:</p>
                    <div style="text-align: center; font-size: 1.3rem; font-weight: bold; margin: 1rem 0; padding: 1rem; background: #ffffff; border-radius: 8px; border: 2px solid #2196f3; color: #0d47a1;">
                        Objective Score = (0.5 √ó Answer Relevancy) + (0.4 √ó Context Precision) + (0.1 √ó Answer Faithfulness)
                    </div>
                    <p style="color: #0d47a1;">This formula prioritizes answer relevancy and context precision while ensuring factual grounding.</p>
                </div>
            </div>

            <div class="section">
                <h2><span class="emoji">üìä</span>Baseline RAG Performance</h2>
                
                <table class="performance-table">
                    <thead>
                        <tr>
                            <th>Metric</th>
                            <th>Mean Score</th>
                            <th>Std Dev</th>
                            <th>Description</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td class="metric-name">Answer Faithfulness</td>
                            <td class="score excellent">0.98</td>
                            <td>¬±0.063</td>
                            <td>Factual consistency with retrieved context</td>
                        </tr>
                        <tr>
                            <td class="metric-name">Answer Relevancy</td>
                            <td class="score good">0.81</td>
                            <td>¬±0.341</td>
                            <td>Direct answer to the query</td>
                        </tr>
                        <tr>
                            <td class="metric-name">Context Precision</td>
                            <td class="score good">0.82</td>
                            <td>¬±0.114</td>
                            <td>Relevance of retrieved documents</td>
                        </tr>
                        <tr style="background: #f8f9fa; font-weight: 600;">
                            <td class="metric-name">Objective Score</td>
                            <td class="score good">0.831</td>
                            <td>¬±0.141</td>
                            <td>Weighted composite performance score</td>
                        </tr>
                    </tbody>
                </table>

                <div class="summary-note">
                    <strong>Evaluation Note:</strong> Summary statistics were obtained by evaluating on <code>train.csv</code>. The full data can be found by running <code>baseline_RAG.ipynb</code>
                </div>
            </div>

            <div class="section">
                <h2><span class="emoji">üöß</span>Problem Analysis</h2>
                
                <div class="problem-analysis">
                    <h4>Biggest Problem</h4>
                    <p><span class="highlight">Answer Relevancy's standard deviation is very high</span>, and the minimum score is 0.</p>
                </div>

                <div class="improvement-area">
                    <h4>Less Urgent Problem</h4>
                    <p><span class="highlight">Context Precision has a relatively low score</span> and standard deviation.</p>
                </div>
            </div>

            <div class="section">
                <h2><span class="emoji">üéØ</span>What Do These Problems Indicate?</h2>
                
                <div class="insights-grid">
                    <div class="insight-card">
                        <h4><span class="emoji">‚ö†Ô∏è</span>Inconsistency in LLM Answers</h4>
                        <p>The high standard deviation indicates that the LLM's ability to provide a relevant answer is <strong>unreliable</strong>. It can produce answers that are completely off-topic (min score of 0.0); we should try to make it more robust and stick to the context.</p>
                    </div>

                    <div class="insight-card">
                        <h4><span class="emoji">üìà</span>Room to Improve</h4>
                        <p>An average of 0.82 in Context Precision indicates that the documents being retrieved are generally relevant and useful for answering the question. But there is still <strong>room to improve the precision</strong>. And this improvement on Context precision can improve the Answer Relevancy score.</p>
                    </div>
                </div>

                <div class="summary-note">
                    <strong>Action Items:</strong> So the place to look at is the <code>generate_answer</code>, <code>add_documents</code> and <code>retrieve</code> method. Detail improvement can be found on <code>improved_RAG</code> notebook.
                </div>
            </div>

            <div class="section">
                <h2><span class="emoji">üí°</span>First Improvement: Advanced Chunk Splitting</h2>
                <p>To address the high variance in <strong>Answer Relevancy</strong>, the chunking strategy was updated. Instead of a simple character-based split, a semantic chunking approach was implemented. This method aims to create more coherent and contextually complete chunks, which should provide better material for the generator model.</p>

                <h3><span class="emoji">üìä</span>Performance After Chunk Splitting</h3>
                <table class="performance-table">
                    <thead>
                        <tr>
                            <th>Metric</th>
                            <th>Mean Score</th>
                            <th>Std Dev</th>
                            <th>Min Score</th>
                            <th>Description</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td class="metric-name">Answer Faithfulness</td>
                            <td class="score excellent">0.95</td>
                            <td>¬±0.126</td>
                            <td>0.60</td>
                            <td>Factual consistency with retrieved context</td>
                        </tr>
                        <tr>
                            <td class="metric-name">Answer Relevancy</td>
                            <td class="score good">0.85</td>
                            <td>¬±0.280</td>
                            <td>0.20</td>
                            <td>Direct answer to the query</td>
                        </tr>
                        <tr>
                            <td class="metric-name">Context Precision</td>
                            <td class="score good">0.81</td>
                            <td>¬±0.110</td>
                            <td>0.60</td>
                            <td>Relevance of retrieved documents</td>
                        </tr>
                        <tr style="background: #f8f9fa; font-weight: 600;">
                            <td class="metric-name">Objective Score</td>
                            <td class="score good">0.844</td>
                            <td>¬±0.109</td>
                            <td>0.60</td>
                            <td>Weighted composite performance score</td>
                        </tr>
                    </tbody>
                </table>
                 <div class="summary-note">
                    <strong>Evaluation Note:</strong> These statistics reflect the performance after updating the chunking strategy. The full data can be found by running <code>improved_RAG.ipynb</code>.
                </div>

                <h3><span class="emoji">üîç</span>Analysis of the Results</h3>
                <div class="insights-grid">
                     <div class="insight-card">
                        <h4><span class="emoji">üëç</span>Why did Answer Relevancy improve?</h4>
                        <p>Answer Relevancy improved because chunking provides the LLM with a <strong>more focused and less noisy context</strong>. By removing irrelevant surrounding text, the model is less likely to get distracted and can more easily generate a direct answer to the query. This resulted in a higher mean score (0.81 ‚ûî 0.85) and more consistent performance (standard deviation dropped from 0.34 to 0.28).</p>
                    </div>
                    <div class="insight-card">
                        <h4><span class="emoji">ü§î</span>Why did Context Precision not change?</h4>
                        <p>Context Precision was stable (~0.81) because the retriever was already finding documents with a <strong>high density of useful sentences</strong>. Chunking simply narrowed the view to a smaller, but equally dense, chunk from that source, keeping the <strong>signal-to-noise ratio consistent</strong>.</p>
                    </div>
                     <div class="insight-card">
                        <h4><span class="emoji">‚ùì</span>Why did Answer Faithfulness slightly decrease?</h4>
                        <p>Answer Faithfulness dropped slightly (0.98 ‚ûî 0.945) because chunking provides the LLM with a more <strong>isolated context</strong>. This process can separate a statement from its supporting details if they land in different chunks. If the retriever only fetches one of these, the LLM's answer may contain claims that aren't verifiable from that specific text, lowering the faithfulness score.</p>
                    </div>
                     <div class="insight-card">
                        <h4><span class="emoji">üìà</span>Conclusion</h4>
                        <p>The chunking strategy successfully improved the average <strong>answer relevancy</strong> to 0.85, but performance remains inconsistent (¬±0.280). Conversely, while average <strong>faithfulness</strong> is high at 0.95, its reliability has decreased (std dev ¬±0.126). This trade-off is a net positive, as it guides future work to specifically target the reduction of this variance in both metrics.</p>
                    </div>
                </div>
            </div>
             <div class="section">
                <h2><span class="emoji">üí°</span>Second Improvement: Fusion Retrieval and Reranking</h2>
                <p>To further enhance performance, a more sophisticated retrieval process was implemented. This involved <strong>Reciprocal Rank Fusion (RRF)</strong> to combine results from multiple retrieval queries, followed by a <strong>Cohere Reranker</strong> to re-order the retrieved chunks based on their relevance to the query. This two-step process aims to improve both the quality and relevance of the context provided to the generator.</p>

                <h3><span class="emoji">üìä</span>Performance After Fusion and Reranking</h3>
                <table class="performance-table">
                    <thead>
                        <tr>
                            <th>Metric</th>
                            <th>Mean Score</th>
                            <th>Std Dev</th>
                            <th>Min Score</th>
                            <th>Description</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td class="metric-name">Answer Faithfulness</td>
                            <td class="score excellent">0.995</td>
                            <td>¬±0.016</td>
                            <td>0.95</td>
                            <td>Factual consistency with retrieved context</td>
                        </tr>
                        <tr>
                            <td class="metric-name">Answer Relevancy</td>
                            <td class="score good">0.84</td>
                            <td>¬±0.310</td>
                            <td>0.00</td>
                            <td>Direct answer to the query</td>
                        </tr>
                        <tr>
                            <td class="metric-name">Context Precision</td>
                            <td class="score excellent">0.84</td>
                            <td>¬±0.097</td>
                            <td>0.70</td>
                            <td>Relevance of retrieved documents</td>
                        </tr>
                        <tr style="background: #f8f9fa; font-weight: 600;">
                            <td class="metric-name">Objective Score</td>
                            <td class="score good">0.856</td>
                            <td>¬±0.114</td>
                            <td>0.60</td>
                            <td>Weighted composite performance score</td>
                        </tr>
                    </tbody>
                </table>
                 <div class="summary-note">
                    <strong>Evaluation Note:</strong> These statistics reflect the performance after adding Fusion Retrieval and a Reranker. The full data can be found by running <code>improved_RAG.ipynb</code>.
                </div>

                <h3><span class="emoji">üîç</span>Analysis of the Results</h3>
                <div class="insights-grid">
                     <div class="insight-card">
                        <h4><span class="emoji">üöÄ</span>Why did Faithfulness improve so much?</h4>
                        <p><strong>Fusion Retrieval</strong> casts a wider, more effective net, while the <strong>Reranker</strong> acts as a powerful quality filter. This synergy provides the LLM with an extremely accurate and focused context, which is why Faithfulness became nearly perfect</p>
                    </div>
                    <div class="insight-card">
                        <h4><span class="emoji">‚ùì</span>Why did Answer Relevancy slightly decrease?</h4>
                        <ul style="margin-top: 0.5rem; margin-left: 1.2rem;">
                            <li style="margin-bottom: 0.5rem;">The retriever is now providing near-perfect context, which proves the issue is not the retriever but the <strong>generator's instructions</strong>.</li>
                            <li style="margin-bottom: 0.5rem;">The simple prompt in your generate_answer function lacks the strict rules needed to keep the LLM focused.</li>
                            <li>This allows it to occasionally get distracted and produce an off-topic response, causing the minimum score to drop to 0.0.</li>
                        </ul>
                    </div>
                    <div class="insight-card">
                        <h4><span class="emoji">ü§î</span>Why did Context Precision stall?</h4>
                        <ul style="margin-top: 0.5rem; margin-left: 1.2rem;">
                            <li style="margin-bottom: 0.5rem;">The score is now capped by our chunking method.</li>
                            <li style="margin-bottom: 0.5rem;">A retrieved chunk may contain the key sentence but also irrelevant filler text, diluting its precision.</li>
                            <li>Further improvement requires more granular chunking that isolates individual facts, creating purer units for the retriever to find.</li>
                        </ul>
                    </div>
                     <div class="insight-card">
                        <h4><span class="emoji">üìà</span>Conclusion</h4>
                        <ul style="margin-top: 0.5rem; margin-left: 1.2rem;">
                            <li style="margin-bottom: 0.5rem;">The gains in <strong>Answer Faithfulness</strong> are profound. This proves that the combination of Fusion Retrieval and Reranking delivers a highly accurate and relevant context to the generator.</li>
                            <li>However, to improve Context Precision and Answer Relevancy, we need further hyperparameter tuning and improvements in the chunking technique.</li>
                        </ul>
                    </div>
                </div>
            </div>
            <div class="section">
                <h2><span class="emoji">‚öôÔ∏è</span>Third Improvement: Semantic Chunking and Hyperparameter Tuning</h2>
                <p>To achieve the best possible RAG system, we introduced two key enhancements: advanced <strong>semantic chunking</strong> and rigorous <strong>hyperparameter tuning</strong>. To guide this tuning process, we first needed to define what constitutes a "Good" RAG system by establishing a clear objective function.</p>



                <h3><span class="emoji">üèÜ</span>Best Hyperparameters Found</h3>
                <p>After running the hyperparameter tuning process, we identified the optimal set of parameters that maximized our objective score:</p>
                
                <div class="insights-grid">
                    <div class="insight-card">
                        <h4>Best Parameters</h4>
                        <ul style="list-style-type: none; padding-left: 0;">
                            <li><strong>Breakpoint Threshold:</strong> 97</li>
                            <li><strong>Initial K (retrieval):</strong> 20</li>
                            <li><strong>Final K (reranking):</strong> 10</li>
                            <li><strong>Temperature (generation):</strong> 0.1</li>
                        </ul>
                    </div>
                    <div class="insight-card">
                        <h4>üöÄ Best Score Achieved</h4>
                        <p style="font-size: 2.5rem; text-align: center; font-weight: bold; color: #27ae60; margin-top: 1rem;">0.9620</p>
                    </div>
                </div>

                <h3><span class="emoji">üìä</span>Final Performance Metrics</h3>
                <table class="performance-table">
                    <thead>
                        <tr>
                            <th>Metric</th>
                            <th>Mean Score</th>
                            <th>Description</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td class="metric-name">Answer Faithfulness</td>
                            <td class="score excellent">0.98</td>
                            <td>Factual consistency with retrieved context</td>
                        </tr>
                        <tr>
                            <td class="metric-name">Answer Relevancy</td>
                            <td class="score excellent">1.00</td>
                            <td>Direct answer to the query</td>
                        </tr>
                        <tr>
                            <td class="metric-name">Context Precision</td>
                            <td class="score excellent">0.91</td>
                            <td>Relevance of retrieved documents</td>
                        </tr>
                        <tr style="background: #f8f9fa; font-weight: 600;">
                            <td class="metric-name">Objective Score</td>
                            <td class="score excellent">0.962</td>
                            <td>Weighted composite score</td>
                        </tr>
                    </tbody>
                </table>
                
                <div class="summary-note">
                    <strong>Final Achievement:</strong> The hyperparameter tuning process successfully optimized our RAG system, achieving an objective score of <strong>0.962</strong>. This represents a significant improvement from the baseline score of 0.831, demonstrating the effectiveness of semantic chunking, fusion retrieval, reranking, and careful hyperparameter optimization.
                </div>

                <h3><span class="emoji">üìä</span>Hyperparameter Importance</h3>
                <p>The following chart illustrates the relative importance of each hyperparameter on the final score. This helps us understand which parameters have the most impact on performance.</p>
                <div style="text-align: center; margin-top: 2rem;">
                    <img src="Hyperparameter_importance.png" alt="Hyperparameter Importance" style="max-width: 100%; border-radius: 12px; box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);">
                </div>
            </div>
        </div>
    </div>
</body>
</html>